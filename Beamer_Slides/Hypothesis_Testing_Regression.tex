\documentclass[t]{beamer}
\usetheme{Copenhagen}
\setbeamertemplate{headline}{} % remove toc from headers
\beamertemplatenavigationsymbolsempty

\usepackage{amsmath, array, tikz, bm, pgfplots, tcolorbox, graphicx, venndiagram, color, colortbl, xfrac}
\pgfplotsset{compat = 1.16}
\usepgfplotslibrary{statistics}
\usetikzlibrary{calc}

\title{Hypothesis Testing}
\subtitle{Regression}
\author{}
\date{}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Objectives}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\begin{frame} 
\maketitle
\end{frame}

\section{Perform a hypothesis test on the slope of the regression equation}

\begin{frame}{Slope of a Line}
Recall that the slope of a line tells us how much the $y$-coordinates change by increasing each $x$-coordinate by 1.	\newline\\	\pause

Previously, we looked at calculating regression equations in the form
\[
\hat {y} = mx + b
\]
\newline
which we can use to predict data values both inside and outside our dataset.	\newline\\	\pause

Also, recall that the coefficient of determination, $r^2$, tells us the prediction error decrease we get by using the linear regression equation, as opposed to the mean of the $y$-coordinates.
\end{frame}

\begin{frame}{Using the $y$-Intercept}
However, we might also be able to use the $y$-intercept of our linear regression equation as a predictor:
\[ y = b\]	\pause

In this case, the slope of the equation, $m$, is 0. \newline\\	\pause

We can then test whether or not the linear regression model we obtained contributes to predicting values of $y$.
\end{frame}

\begin{frame}{Hypotheses Regarding Slope}
For this hypothesis, testing, we are operating under the assumption that the following null hypothesis is true:
\begin{center}
The slope of the regression equation does not help in predicting values of $y$.
\end{center}	\pause
To put it mathematically:
\[
H_0: m = 0
\]
\pause
Our alternative hypothesis will be one of the forms
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Left-Tailed} & \textbf{Right-Tailed} & \textbf{Two-Tailed}  \\ \hline 
$m < 0$ & $m > 0$ & $m \neq 0$ \\ \hline
\end{tabular}	\pause	\newline\\
$H_A$ states that $x$ is not helpful in predicting $y$.
\end{center}
\end{frame}

\begin{frame}{Test Statistic}
Since the population standard deviation of our regression slope is likely not to be known, we can estimate it by using the {\color{blue}\textbf{estimated standard error of the least squares slope}}.	\pause
\[
\frac{s}{\sqrt{SS_{xx}}}
\]
where 
\[
SS_{xx} = \text{total sum of squares} = \sum x^2 - \frac{1}{n}\left(\sum x\right)^2
\]
\pause

Our test statistic, $t$, is thus
\[
t = \frac{\hat{m}}{s/\sqrt{SS_{xx}}}
\]
where $\hat{m}$ is the slope of our regression equation.
\end{frame}

\begin{frame}{$p$-Value}
The area under the curve either to the left, right, or outside of our test statistic(s), i.e. the $p$-value, can be found by taking $n-2$ degrees of freedom with that test statistic.
\end{frame}

\begin{frame}{Confidence Intervals}
We can construct confidence intervals as
\[
\hat{m} \pm t_{\alpha/2}\left(s/\sqrt{SS_{xx}}\right)
\]
\pause

Remember, for one-sided confidence intervals, use $t_{\alpha}$ instead of $t_{\alpha/2}$.
\end{frame}

\begin{frame}{Residuals Review}
Recall that the {\color{blue}\textbf{residual}} of a data point is the distance the $y$-coordinate of that data point is from the corresponding $y$-coordinate on the regression equation. \newline\\	\pause

We usually denote the residual of data point $(x_i, \, y_i)$ as $\epsilon_i$, and it can be found by 
\[
\epsilon_i = y_{\text{observed}} - y_{\text{predicted}}
\]
\pause

When performing hypothesis testing regarding the slope of the regression equation, we operate under the following assumptions:
\end{frame}

\begin{frame}{You Know What Happens When You Assume, Right?}
\textbf{Assumptions:}
\begin{itemize}
	\item<2-> The mean of all values of $\epsilon$ is 0.
	\item<3-> The variance of all values of $\epsilon$ is constant for all values of $x$.
	\item<4-> The distribution of $\epsilon$ is normal.
	\item<5-> Values of $\epsilon$ associated with any two values of $y$ are independent.
\end{itemize}
\end{frame}

\begin{frame}{Example 1}
The table below lists the total study times $x$, in hours, and the score on a statistics exam $y$ for 12 students. \newline\\
(a) \quad Find the least squares regression equation for the data and interpret the slope.	\newline\\

\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Time} & 1.9 & 1.3 & 1.7 & 1.2 & 1.2 & 2.7 & 1.7 & 2.7 & 2.0 & 1.8 & 1.4 & 2.4 \\ \hline
\textbf{Score} & 84 & 81 & 86 & 82 & 77 & 97 & 93 & 96 & 89 & 89 & 88 & 95 \\ \hline
\end{tabular}
}
\onslide<2->{\[\hat{y} = 10.3158x + 69.1711\]}
\onslide<3->{\[\text{Score} = 10.3158\left(\text{Time}\right)+69.1711\]}
\onslide<4->{For every increased hour in study time, the predicted score increases by about 10.3}
\end{frame}

\begin{frame}{Example 1}
(b) \quad At the 5\% significance level, test the claim whether study time is useful as a predictor of test score.	\newline\\	\pause
$H_0: \, m = 0$ (study time is not useful) \newline \pause
$H_A: \, m \neq 0$ (study time is useful) \newline\\	\pause

t = 5.5365 (critical values: $\pm 2.2281$)	\newline
p-value: 0.0002 ($\alpha = 0.05$) \newline
95.0\% confidence interval for population slope: (6.1643, 14.4673)	\newline\\	\pause
\textbf{Reject the null hypothesis} \pause
\begin{quote}
At the 5\% significance level, we have sufficient evidence to reject the claim that study time is not useful in predicting the score on this statistics exam.
\end{quote}
\end{frame}

\section{Perform hypothesis tests on the linear correlation coefficient}
% Inferences on correlation

\section{Create a prediction interval for a regression equation}
% Prediction interval for new values given regression equation
% Graph of prediction interval



\end{document}